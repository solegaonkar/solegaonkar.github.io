<!DOCTYPE html><html><head><title>Independent Component Analysis</title><script src='scripts/index.js'></script></head><body><h1>Independent Component Analysis</h1><hr/>

<p>ICA is a dimensionality reduction algorithm similar to the PCA. But, it borrows from information theory more than statistics. PCA tries to generate components that are orthogonal. But ICA focuses on identifying components that are independent.</p><p>To illustrate with an example, there is a significant correlation between the age of a person and the number of calories he burns in a day. They are not orthogonal. Yet they are independent. Just as PCA tries to identify latent variables that are orthogonal, ICA proposes that all the parameters are a linear mixture of latent parameters that are independent.</p><p>It makes a lot more sense to look for independent parameters instead of orthogonal parameters. Independent variables are the ones that change independently. Although their values move together (implying they are not orthogonal), if one variable changes independent of the other, they are independent.</p><p>Independent parameters are sure to give us a better models. Because independent variation is what matters most.</p><h3>Python Code</h3><hr/><p>It is quite easy to implement the Independent component analysis using Python code. SktLearn provides us a simple functions to do it.</p><pre><code class='python'>from sklearn.decomposition import FastICA 
ICA = FastICA(n_components=3, random_state=12) 
X=ICA.fit_transform(df[feat_cols].values)</code></pre>

</body><script>loadPageFormat();</script></html>
