<!DOCTYPE html><html><head><title>Reducing Variance</title><script src='scripts/index.js'></script></head><body><h1>Reducing Variance</h1><hr/><p><a href="https://thewiz.net"><h4>TheWiz.Net</h4></a></p>

<p>The error analysis can point out the major cause of the error. If high variance is the problem, we can use one of these techniques to reduce that.</p><h4>Add more training data</h4><hr/><p>This is the primary solution. Variance is caused when we do not have enough data to train the network to its best performance. So the primary action point should be looking out for more data. But this has its limits as the data is not always available.</p><h4>Add Regularization</h4><hr/><p>L1 or L2 regularization are proven techniques for reducing the problem of overfitting - and thus avoiding high variance. Essentially, they hold each parameter closer to 0. That means, no parameter is allowed to learn too much. If a single parameter holds a lot of information, the model gets imbalanced and leads to overfitting and high variance.</p><p>The L1 and L2 regularization techniques help prevent such problems.</p><h4>Early Stopping</h4><hr/><p>As we train the model with the available training data, each iteration makes the model a little better for the data available. But, having excessive number iterations of this can cause overfitting. One has to find the golden mean for this. The best way is to stop early - rather than realizing that we have already crossed the limits.</p><h4>Reduce Features</h4><hr/><p>Lesser the number of features, lighter is the model and hence lesser the scope for overfitting. We have several feature selection algorithms like PCA that can help us identify minimal and orthogonal features that can provide an easier way to train the models. Using the domain knowledge can also help us reduce the number of features. We can also use the insights from the error analysis to identify how the feature set should be altered in order to get a better performance.</p><h4>Decrease the model size</h4><hr/><p>High variance or Overfitting typically means that we have too many parameters to train. If we do not have enough data to train each of these parameters, the randomness of the initialization values remains in the parameters - leading to incorrect results.</p><p>Reducing the model size has a direct impact on it.</p><h4>Use Sparse Model</h4><hr/><p>Sometimes, we know that the model size is imperative and reducing the size would only reduce the functionality. In such a case, we can consider training a sparse model. That gives an good combination of better model with lesser variance.</p><h4>Model Architecture</h4><hr/><p>Researchers have faced and worked these problems in past and provided us with good model architectures that can be used to give a better tradeoff between the bias and variance. Aligning to such an architecture can help us prevent a lot of our problems.</p>

</body><script>loadPageFormat();</script></html>
