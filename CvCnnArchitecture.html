<!DOCTYPE html><html><head><title>CNN Architecture</title><script src="scripts/index.js"></script></head><body><h1>CNN Architecture</h1><hr/><p>Typical small or medium size CNN models follow some basic principles.</p><div class='center'><img src='img/typical_cnn.png' /></div><ul><li>Alternate convolution and pooling layers</li><li>Gradually decreasing frame size and increasing frame count, </li><li>Flat and fully connected layers towards the end</li><li>RelU activation for all hidden layers, followed by a softmax for the final layer</li></ul><p>A prominent concept in CNN architectures is that the alternate layers change the information content to sparse and dense one after the other. This helps separate the individual pieces of the information.  One can think of this as someone playing with a cotton ball. If we pull and push the threads again and again, we naturally separate the individual threads. Similarly, a CNN can separate individual components in the image.</p><p>Things get more and more complex as we move over to large and very large networks. Researchers have provided us more concrete architectures that we can use here. <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet</a>, <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">GoogleNet</a> and <a href="https://arxiv.org/pdf/1409.1556v6.pdf">VGGNet</a> are a few of these.</p></body><script>loadPageFormat();</script></html>