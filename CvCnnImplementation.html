<!DOCTYPE html><html><head><title>CNN Architecture</title><script src="scripts/index.js"></script></head><body><h1>CNN Architecture</h1><hr/><p>Typically implementation of a CNN model data analysis and cleanup, followed by choosing a network model that we can start with. We provide the architecture in terms of the layout of the network number and size of layers and their connectivity - then we allow the network to learn the rest for itself. We can then tweak the hyperparameters to generate a model that is good enough for our purpose.</p><p>Let us check out a simple example of how a convolutional network would work. In a <a href="https://towardsdatascience.com/a-glimpse-of-tensorflow-bd9c6c06ab73">previous blog</a>, we had a look at building the MNIST model with a fully connected neural network. You can check it out if you want to have a detailed view of how TensorFlow and Keras can be used to build a deep model. Let us now look at doing the same job with a Convolutional Network.</p><h4>Import the Modules</h4><hr/><p>We start by importing the required modules.</p><pre><code class='python'>import numpy as np

import tensorflow as tf
from tensorflow import keras

from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D
from keras.models import Sequential</code></pre><h4>Get the Data</h4><hr/><p>The next step is to get the data. For academic purpose, we use the data set build into the Keras module - the MNIST data set. In real life, this would require a lot more processing. For now, let us proceed with this.</p><p>Thus, we have the train and test data loaded. We reshape the data to make it more suitable for the convolutional networks. Essentially, we reshape it to a 4D array that has 60000 (number of records) entries of size 28x28x1 (each image has size 28x28). This makes it easy to build the Convolutional layer in Keras.</p><p>If we wanted a dense neural network, we would reshape the data into 60000x784 - a 1D record per training image. But CNN's are different. Remember that concept of convolution is 2D - so there is no point flattening it into a single dimensional array.</p><p>We also change the labels into a categorical one-hot array instead of numeric classification. And finally, we normalize the image data to ensure we reduce the possibility of <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing gradients</a>.</p><pre><code class='python'>(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

train_images = train_images.reshape(60000,28,28,1)
test_images = test_images.reshape(10000,28,28,1)

test_labels = tf.keras.utils.to_categorical(test_labels)
train_labels = tf.keras.utils.to_categorical(train_labels)

train_images = train_images / 255.0
test_images = test_images / 255.0</code></pre><h4>Build the Model</h4><hr/><p>The Keras library provides us ready to use API to build the model we want. We begin with creating an instance of the Sequential model. We then add individual layers into the model. The first layer is a convolution layer that processes input image of 28x28. We define the kernel size as 3 and create 32 such kernels - to create an output of 32 frames - of size 26x26 (28-3+1=26)</p><p>This is followed by a max pooling layer of 2x2. This reduces the dimensions from 26x26 to 13x13. We used max pooling because we know that the essence of the problem is based on edges - and we know that edges show up as high values in a convolution.</p><p>This is followed by another convolution layer with kernel size of 3x3, and generates 24 output frames. The size of each frame is 22x22. It is again followed by a convolution layer. Finally, we flatten this data and feed it to a dense layer that has outputs corresponding  to the 10 required values.</p><pre><code class='python'>model = Sequential()
model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(24, kernel_size=3, activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(10, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</code></pre><h4>Train the Model</h4><hr/><p>Finally, we train the model with the data we have. Five epochs are enough to get a reasonably accurate model.</p><pre><code class='python'>model.fit(train_images, train_labels, validation_data=(test_images, test_labels), epochs=5)

Train on 60000 samples, validate on 10000 samples
Epoch 1/5
60000/60000 [==============================] - 49s 817us/step - loss: 0.2055 - acc: 0.9380 - val_loss: 0.0750 - val_acc: 0.9769
Epoch 2/5
60000/60000 [==============================] - 47s 789us/step - loss: 0.0776 - acc: 0.9762 - val_loss: 0.0493 - val_acc: 0.9831
Epoch 3/5
60000/60000 [==============================] - 47s 789us/step - loss: 0.0557 - acc: 0.9830 - val_loss: 0.0489 - val_acc: 0.9825
Epoch 4/5
60000/60000 [==============================] - 47s 787us/step - loss: 0.0441 - acc: 0.9864 - val_loss: 0.0493 - val_acc: 0.9845
Epoch 5/5
60000/60000 [==============================] - 47s 788us/step - loss: 0.0367 - acc: 0.9885 - val_loss: 0.0348 - val_acc: 0.9897
&gt;keras.callbacks.History at 0x7fbf95e33e48&lt;</code></pre></body><script>loadPageFormat();</script></html>