<!DOCTYPE html><html><head><title>Neural Networks</title><script src='scripts/index.js'></script></head><body><h1>Neural Networks</h1><hr/><p><a href="">TheWiz.Net</a></p>

<p>From the mathematical point of view, there is a limit to what a linear function can do and it was noticed that results of polynomials were not good enough to justify the computational expense. Hence the concept of neurons picked up momentum. A lot of Machine Learning is inspired by how the human mind works. The concept of Neural Networks goes one step further - to take inspiration from the way Neurons are laid out in the human brain.</p><div class="center"><img src='img/neuron.png' /></div><p>As you can see in the image above, the neurons get multiple inputs and based on these, give out one output that feeds into another set of neurons, and so on. The nervous system is built of many such neurons connected to each other. Each neuron contributes to the decision process by appropriately forwarding the input signal - based on the training it has gathered. Such a model has the potential to hold all that a human brain does. Each neuron has a minimal functionality that can potentially do wonders.</p><p>Neurons are implemented as linear function with a non linear topping - called the activation function. Thus, each neuron is defined by weights for each input and a bias. The result of this operation is fed into the activation function. The final output is the input for the next set of neurons. Such Artificial neuron is called a Perceptron.</p><div class='center'><img src='img/perceptron.png' /></div><p>Often the network has multiple layers of such Perceptrons. That is called MLP (Multy Layer Perceptron). In an MLP, we have an input layer, an output layer and zero or more hidden layers.</p><h3>Network of Perceptrons</h3><hr/><p>Each Perceptron has an array of inputs and an array of weights that are multiplied with the inputs to generate a scalar. This processing is linear - they cannot help fitting a non linear curve - irrespective of the depth of the network. If the network has to fit non linear curves, we need some non linear element on each perceptron. Hence, perceptrons are tipped with a non linear activation function. This could be a sigmoid or tanh or relu ... Researchers have offered several activation functions that have specific advantages.</p><p>With everything in place, a neural network looks like this:</p><div class='center'><img src='img/mlp_one_hidden_layer.png' /></div><p>The layout, width and depth of the network is one of the most interesting topics of research. Experts have developed different kinds of networks for different kinds of problems. The deeper and larger the network, more is its capacity. Human brain has around 100 billion neurons. Neural Networks are nowhere near that - some researchers quote experiments with million. This concept of large neural networks or Deep Learning is not new. But it was limited to mathematical curiosity and research papers. The recent boom in the availability of massive training data and computing power has made it a big success.</p><p>Building, training and tuning Neural Networks is a massive domain and deserves many blogs dedicated to each topic.</p><p>But even small networks with very few neurons are capable of performing some minor tasks. Let us look at one such task implemented with ScikitLearn.</p><h2>Python Implementation</h2><hr/><p>We have several open source libraries like ScikitLearn, Tensorflow and Keras that can help us implement Neural Networks is just a couple of lines of code. </p><ul><li><a href='NeuralNetworksScikitLearn.html' class='link'>ScikitLearn</a>: ScikitLearn was good to demonstrate small examples. But it does not perform well enough to be used in bigger Neural Networks. Tensorflow helps handle Deep Neural Networks.</li><li><a href='TensorFlow.html' class='link'>Tensorflow</a>: Here we will go through a small real life classification problem to show how we use TensorFlow to handle it.</li></ul><p>These were simple examples using small models. Neural Networks have a lot more in store for us. But that requires larger networks. As the network size increases, their efficiency seems to decrease and we need to understand a lot more to get them to work as we want them to. Deep Learning deals with these aspects of Neural Networks.</p>

</body><script>loadPageFormat();</script></html>
