<!DOCTYPE html><html><head><title>Common Sources of Error</title><script src='scripts/index.js'></script></head><body><h1>Common Sources of Error</h1><hr/><p><a href="">TheWiz.Net</a></p>

<p>There could be several sources of these errors. Every model would have its own unique sources of errors. And we need to look at them individually. But, the typical causes are:</p><h4>Mislabeled Data</h4><hr/><p>Most of the data labeling is traced back to humans. We may extract data from the net or surveys or various other sources. The basic inputs came from humans. And humans are error prone. Thus, we should acknowledge the fact that all our train/dev/test data has some mislabeled records. If our model is well built and trained properly, then it should be able to overcome such errors.</p><h4>Hazy Line of Demarcation</h4><hr/><p>Classification algorithms work well when the positive and negative are clearly separated. For example, if we are trying to classify images of an ant and a human; the demarcation is pretty good and that should help speed up the training process.</p><p>But, if we want to classify between a male and female hairstyle, it is not so simple. We know the extremes very well. The demarcation is not so clear. Such classification is naturally error prone. In such a case, we have to work on a better training near this hazy line of demarcation.</p><h4>Overfitting or Underfitting a Dimension</h4><hr/><p>Let us consider a trivial example just to understand the concept. Suppose we are working on an image classifier to distinguish between a crow and a parrot. Apart from the size, beak, tail, wings.. the obvious differentiator is the color. But it is possible somehow the the model does not learn this difference. Thus, classifies a baby crow as a parrot.</p><p>That means, the model failed to learn a dimension from the available data. When we notice this, we should try to gather more data that can train the network to classify based on the color more than other parameters.</p><p>Similarly, it is possible that the model overfits a particular dimension. Suppose in a Cat/Dog classifier, we notice in the error records that a lot of dark dogs were classified as cats and light cats were classified as dogs. This means, the training data did not have enough records that could train the model against such misclassification.</p><h4>Many Others</h4><hr/><p>These are just a few kinds of error sources. There could be many more - that one can discover on analyzing the error set. Let us not "Overfit" our understanding to limit our analysis to these types of error. Every error analysis will show us a new set of problem sources.</p>

</body><script>loadPageFormat();</script></html>
