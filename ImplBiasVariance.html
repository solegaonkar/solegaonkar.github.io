<!DOCTYPE html><html><head><title>Bias &amp; Variance</title><script src='scripts/index.js'></script></head><body><h1>Bias &amp; Variance</h1><hr/>

<p>As we work on error analysis, we identify a particular parameter or area of problems; or we notice that the error is pretty uniform. How do we go about from here? Do I get more data? It may sound logical. But not always true. More data may not always help - beyond a point, the data could be just redundant. Do I need more data? Do I need a richer model? Just enriching the model can greatly improve the numbers - by over-fitting. That is not right either! So how do we decide on the direction?</p><p>The bias and variance give us a good insight into this. In simple words, if the error is high in the training set as well as dev set, then we have high bias. While if the training set is good but dev set is bad, we have high variance. Bias essentially implies that the output is bad for all data. Variance implies that the output is good for some data and bad for the rest.</p><p>If we have a model with 70% accuracy on the training set. Naturally we call it a high bias. With this kind of accuracy, we may not even want to check up the dev set. But, if the training set error is much better than our target, we can call it high variance. That is because, the behavior of the model varies heavily over the available data.</p><p>One can intuitively say that if we have a high bias, it means we are underfitting. This could be because a particular feature is not processed properly, or the model itself not rich enough. Based on this, we can update the solution to improve the performance - by enhancing the particular feature or the model itself.</p><p>On the other hand, high variance means we are not training it enough. We need more data and we need to train it better.</p>

</body><script>loadPageFormat();</script></html>
