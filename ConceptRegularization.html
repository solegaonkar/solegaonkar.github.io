<!DOCTYPE html><html><head><title>Regularization</title><script src='scripts/index.js'></script></head><body><h1>Regularization</h1><hr/>

<p>Among all the techniques for avoiding Overfitting, weight penalty or regularization is the most commonly used. Regularization is one of the common approaches to avoid overfitting - by preventing any particular weight from growing too high. Please not that using regularization does not guarantee a better model. In fact it can also degrade the model.</p><p>The amount of regularization - the weight penalty - is one of the many "Hyper Parameters". At times, it helps to play around with different values of Hyper-Parameters. Overall, their choice is what defines the success of a given model. This takes a lot of experience.</p><h3>Types of Regularization</h3><hr/><p>There are two main types of Regularization based on how the weights are penalized:</p><h4>L1 Regularization</h4><p>Here, the weights are penalized based on the absolute values of the weights. (L1 because it uses the values and not their squares as in L2). L1 Regularization tends to produce a sparce model by discarding a lot of weights that are not so important. This may not result in a model as good as L2; but the performance is a lot better.</p><h4>L2 Regularization</h4><p>Here, the weights are penalized based on the sum of squares rather than the absolute values. (L2 because it uses the squares rather than the value). L2 Regularization tends to produce a dense model by lowering the weights. Hence the model is a lot better but the performance is not so good.</p><p>ScikitLearn uses L2 by default.</p><h3>Comparison</h3><hr/><p>When we have a choice, we certainly want to compare the two and identify which one is better. To do this, we can implement a simple logistic regression problem to classify using the standard breast cancer database. This can give us an idea about their relative performance.</p><p>We start with creating two instances of the LogisticRegression class</p><pre><code class='python'>lr1 = LogisticRegression(C=0.01, penalty='l1', tol=0.01)
lr2 = LogisticRegression(C=0.01, penalty='l2', tol=0.01)</code></pre><p>Next, we can train either of them using our train data.</p><pre><code class='python'>lr1.fit(X_train, y_train)
lr2.fit(X_train, y_train)</code></pre><p>Now that we have trained the two models, we can go ahead and check out how they performed.</p><pre><code class='python'>print('L1: Accuracy on the training subset: {:.3f}'.format(lr1.score(X_train, y_train)))
print('L1: Accuracy on the test subset: {:.3f}'.format(lr1.score(X_test, y_test)))
print('L2: Accuracy on the training subset: {:.3f}'.format(lr2.score(X_train, y_train)))
print('L2: Accuracy on the test subset: {:.3f}'.format(lr2.score(X_test, y_test)))</code></pre><p>When we do this, we see the result like this. Please note that the numbers may not be the same every time we run this code. But the values would be similar.</p><pre><code class='python'>L1: Accuracy on the training subset: 0.918
L1: Accuracy on the test subset: 0.930
L2: Accuracy on the training subset: 0.925
L2: Accuracy on the test subset: 0.930</code></pre><p>As we can see here, the values are quite similar in either case. Let us now check out how the model coefficients look.</p><pre><code class='python'>np.mean(lr1.coef_.ravel()==0)
np.mean(lr2.coef_.ravel()==0)</code></pre><p>Here we see a major difference between the two models.</p><pre><code class='python'>0.8666666666666667
0.0</code></pre><p>As we can see, almost 87% of the weights are stripped off in the L1 regularization.</p><h3>Alter the Penalty</h3><hr/><p>That was because we used C = 0.01 - implying a very high penalty. If we change it to C = 100, it would reduce the penalty significantly.</p><pre><code class='python'>lr1 = LogisticRegression(C=100, penalty='l1', tol=0.01)
lr2 = LogisticRegression(C=100, penalty='l2', tol=0.01)</code></pre><p>Now, if we train these models, and check the accuracy again, we see something like this:</p><pre><code class='python'>L1: Accuracy on the training subset: 0.984
L1: Accuracy on the test subset: 0.979
L2: Accuracy on the training subset: 0.927
L2: Accuracy on the test subset: 0.930</code></pre><p>In this case, the accuracy of the training as well as the test set increased with reduction in the penalty. Now, if we check out the coefficients again, we see this:</p><pre><code class='python'>0.10000000000000001
0</code></pre><p>Only 10% of the L1 coefficients are stripped off.</p><p>That is to remind us that regularization will not always improve the model. High regularization will actually degrade the model.</p>

</body><script>loadPageFormat();</script></script></html>
