<!DOCTYPE html><html><head><title>Principle Component Analysis</title><script src='scripts/index.js'></script></head><body><h1>Principle Component Analysis</h1><hr/><p><a href="">TheWiz.Net</a></p>

<p>PCA is an interesting algorithm for dimensionality reduction. It is based on several of the techniques we discussed above. The various different parameters available to us are normally not orthogonal. That means, there is a considerable correlation between them. Yet, they are not close enough to discard one. For such a problem, Factor Analysis tries to identify lesser number of latent parameters that can carry almost all the information.</p><p>On the other hand, PCA tries to generate a large number of parameters with decreasing significance.</p><p>A principal component is a linear combination of the available features. The first principle component explains most of the variance in the data set. The second principle component tries to explain the variance that was not explained by the first. The third tries to explain the variance that was not explained by the previous two.. and so on.</p><p>Once we have discovered these components, we can be sure of the order of importance. And we also know that are each of these components is orthogonal. It is a lot easier to work with them.</p><h3>Python Code</h3><hr/><p>Thanks to the utilities provided by libraries like SktLearn, implementing PCA is not a big deal.</p><pre><code class='python'>from sklearn.decomposition import PCA
pca = PCA(n_components=4)
pca_result = pca.fit_transform(df[feat_cols].values)</code></pre><p>The number of components we choose depends upon how accurate we want it to be. It is another hyperparameter that we need to optimize in the process of training.</p>

</body><script>loadPageFormat();</script></html>
