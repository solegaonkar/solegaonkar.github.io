<!DOCTYPE html><html><head><title>Backward &amp; Forward Selection</title><script src='scripts/index.js'></script></head><body><h1>Backward &amp; Forward Selection</h1><hr/><p><a href="">TheWiz.Net</a></p>

<p>This dimensionality reduction algorithm may not seem very exciting. It is the hard way of doing things. Here, we take a very small subset of the training data and try to use it for feature selection. We try to train the model using only a few of the available features and identify the amount of impact a given feature makes.</p><p>In forward selection, we start with just one feature - identifying upward which features makes most impact. In backward selection, we go the other way - eliminating features one by one to identify features that make almost no difference.</p><p>These seem to be mere academic methods. But they can be quite useful if we have a huge data that is reasonably represented by a small subset. It may not offer a major contribution in building the model. But certainly helps in identifying parameters that can be dropped from the prediction and continuous training implementations.</p>

</body><script>loadPageFormat();</script></html>
