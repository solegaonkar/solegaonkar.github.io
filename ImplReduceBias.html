<!DOCTYPE html><html><head><title>Reducing Bias</title><script src='scripts/index.js'></script></head><body><h1>Reducing Bias</h1><hr/><p><a href="https://thewiz.net"><h4>TheWiz.Net</h4></a></p>

<p>A machine learning model can only learn from the data available to it. Some errors are unavoidable in the input data. This are not human mistakes - but true limitations of humans who classify or test the model. For example, if I cannot differentiate between a pair of identical twins, there is no way I can generate labeled data and teach a machine to do it!</p><p>Such a limitation is called unavoidable bias. The rest is avoidable bias - and we need to focus on that. So, when we perform an error analysis, when we try to we should consider the avoidable bias instead of the bias as a whole.</p><h2>Reducing Avoidable Bias</h2><hr/><p>If our error analysis tells us that the avoidable bias is the major source of error, we can try some of the following steps</p><h4>Increase the model size</h4><hr/><p>High bias means the model is not able to learn all that it can learn from the data available to it. This happens when the model is not capable of learning enough. If the model has just two parameters, it cannot learn more than what these two parameters can hold. Beyond that, any new training data will overwrite what it had learnt from the previous records. The model should have enough parameters to learn - only then it can hold the information required to do the job required.</p><p>Hence the primary solution to high bias is to build a richer model.</p><h4>Allow more Features</h4><hr/><p>One of the major steps in our data cleanup is to reduce all the redundant features. In fact, no feature is really redundant. But some are less meaningful than others. And feature reduction essentially discards such features with lesser value.</p><p>That is good to begin with. But, when we notice that features we have are not able to carry the required information, we have to rework the feature reduction step and allow some more features to pass through. That can make the model richer and give it more information to learn from.</p><h4>Reduce Model Regularization</h4><hr/><p>All the regularization techniques essentially hold the model parameters closer to zero. That is, it prevents each parameter from "learning too much". That is a good technique for ensuring the model remains balanced. But, when we realize that the model is not able to hold all the information we provide it, we should reduce the regularization levels - so that each node on the network will be able to learn more from the data available for training.</p><h4>Better Network Architecture</h4><hr/><p>Just increasing the neurons and layers does not necessarily improve the model. Using an appropriate network architecture can make sure the new layers actually add value to it.</p><p>Researchers have faced and worked these problems in past and provided us with good model architectures that can be used to give a better tradeoff between the bias and variance. Aligning to such an architecture can help us prevent a lot of our problems.</p>

</body><script>loadPageFormat();</script></html>
