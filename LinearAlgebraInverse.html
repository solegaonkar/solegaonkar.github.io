<!DOCTYPE html><html><head><title>Matrix Inverse</title><script src='scripts/index.js'></script></head><body><h1>Matrix Inverse</h1><hr/><p><a href="">TheWiz.Net</a></p>

<p>Concept of matrix inverse is very important for most linear algebra problems. Several mathematical libraries are dedicated to just calculating the inverse in an efficient way.</p><h3>Linear Independence &amp; Rank</h3><hr/><p>A set of n vectors {x<sub>1</sub>, x<sub>2</sub>, . . . x<sub>n</sub>} &isin; R<sup>n</sup> is called linearly independent if no vector can be expressed as a combination of the other vectors. Conversely, a set of n vectors is called linearly dependent if at least one of them can be expressed as</p><pre><code class='python'>    x<sub>n</sub> = &sum; &alpha;<sub>i</sub>x<sub>i</sub></code></pre><p>for some scalar values {&alpha;x<sub>1</sub>&alpha;x<sub>2</sub>. . . &alpha;x<sub>n-1</sub>}</p><p>The column rank of a matrix is the largest subset of column vectors that are linearly independent. Similarly, the row rank is the largest subset of row vectors that are linearly independent.</p><p>The rank can be roughly considered as the information contained in the matrix. It can be proved that for any matrix A, the column rank is always equal to the row rank. Hence it is generally referred as the rank of the matrix. Some interesting properties of matrix ranks:</p><ul><li>For a matrix A &isin; R<sup>m x n</sup>, the rank(A) = &lt;= min(m,n). If rank(A) = min(m,n), it is called a full rank matrix.</li><li>For any matrix A, rank(A) = rank(A<sup>T</sup>)</li><li>For any matrix A &isin; R<sup>m x n</sup> and B &isin; R<sup>n x p</sup>, the rank(AB) &lt;= min(rank(A), rank(B))</li><li>For any matrix A,B &isin; R<sup>m x n</sup>, rank(A + B) &lt;= rank(A) + rank(B)</li></ul><h3>Inverse</h3><hr/><p>The inverse of a matrix A &isin; R<sup>n x n</sup> is another matrix A<sup>-1</sup> &isin; R<sup>n x n</sup> such that</p><pre><code class='python'>    A<sup>-1</sup>A = AA<sup>-1</sup> = I<sup>n</sup></code></pre><p>The matrix inverse is unique. A matrix cannot have multiple inverses. Also, the inverse may not be defined for every matrix. It is certainly not defined for non-square matrices. Even with square matrices, the inverse may not be defined. Such a matrix is called Singular Matrix.</p><p>A matrix is called non-singular or invertible if and only if A<sup>-1</sup> exists. Else it is singular or non-invertible. A non-singular or invertible matrix is always a full rank matrix.</p><p>For matrices A, B &isin; R<sup>n x n</sup>, we have</p><ul><li>(A<sup>-1</sup>)<sup>-1</sup> = A</li><li>(AB)<sup>-1</sup> = B<sup>-1</sup>A<sup>-1</sup></li><li>(A<sup>-1</sup>)<sup>T</sup> = (A<sup>T</sup>)<sup>-1</sup>. Hence it is also referred as A<sup>-T</sup></li></ul><p>For the linear equations we saw above,</p><pre><code class='python'>       Ax = b
    =&gt; x = A<sup>-1</sup>b</code></pre><p>That makes the job pretty simple. Of course, for this we require that A is a full rank square matrix. That is, we have as many equations as the number of variables and none of the equations is redundant. These are the necessary and sufficient conditions for a set of linear equations to be solvable.</p><h3>Orthogonal &amp; Orthonormal</h3><hr/><p>For two vectors x, y &isin; R<sup>n</sup>, x<sup>T</sup>y can be considered to be the shadow of x on y (or vice-versa. The dot product is positive if x has some components in the direction of y (or y in the direction of x) and it is negative if x has some components in direction of y (or y in the direction of x). In a sense, it is an indicator of the angle between them. The two vectors x, y &isin; R<sup>n</sup> are orthogonal if this dot product is 0. </p><p>And a vector x &isin; R<sup>n</sup> is called normalized if its L<sub>2</sub> norm is 1. Two vectors are called orthonormal if both are normalized and orthogonal.</p><p>When we talk about matrices, the definition of orthogonality is slightly different - although intuitively, it means the same. A square matrix A &isin; R<sup>n x n</sup> is orthogonal if all its columns are normalized and orthogonal to each other. From this definition, it follows that</p><pre><code class='python'>    UU<sup>T</sup> = I = U<sup>T</sup>U</code></pre><p>In other words, the transpose of an orthogonal matrix is also its inverse.</p><p>Note that we need a square matrix. If A is not a square matrix with orthonormal columns, AA<sup>T</sup> will be I. But since we have more rows than the columns - more rows than the dimensions, they can never by orthonormal. (eg: three vectors in 2D space can never be orthogonal to each other). Thus the A<sup>T</sup>A is not I.</p><p>Thus, we need a square matrix for it to be orthogonal.</p><p>The orthogonal matrices have some interesting properties. For example, when we multiple a vector with an orthogonal matrix, its Euclidean (L<sub>2</sub>) norm remains unchanged. Thus, for an orthogonal matrix U &isin; R<sup>n x n</sup></p><pre><code class='python'>    ||Ux||<sub>2</sub> = ||x||<sub>2</sub></code></pre><p>For any x &isin; R<sup>n</sup></p>

</body><script>loadPageFormat();</script></html>
